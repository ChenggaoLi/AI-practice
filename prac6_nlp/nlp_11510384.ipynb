{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weeks 13-14\n",
    "\n",
    "## Natural Language Processing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import pandas to read in data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import models and evaluation functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Import vectorizers to turn text into numeric\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import plotting\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Feature Engineering\n",
    "We have examined two ways of dealing with categorical (i.e. text based) data: binarizing/dummy variables and numerical scaling. \n",
    "\n",
    "See the following examples for implementation in sklearn to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/categorical.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Minutes</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Marital</th>\n",
       "      <th>Satisfaction</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Male</td>\n",
       "      <td>Single</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220</td>\n",
       "      <td>Female</td>\n",
       "      <td>Married</td>\n",
       "      <td>Very Low</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>Female</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>Male</td>\n",
       "      <td>Single</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>450</td>\n",
       "      <td>Male</td>\n",
       "      <td>Married</td>\n",
       "      <td>Very High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Minutes  Gender   Marital Satisfaction  Churn\n",
       "0      100    Male    Single          Low      0\n",
       "1      220  Female   Married     Very Low      0\n",
       "2      500  Female  Divorced         High      1\n",
       "3      335    Male    Single      Neutral      0\n",
       "4      450    Male   Married    Very High      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizing\n",
    "Get a list of features you want to binarize, go through each feature and create new features for each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_binarize = [\"Gender\", \"Marital\"]\n",
    "\n",
    "# Go through each feature\n",
    "for feature in features_to_binarize:\n",
    "    # Go through each level in this feature (except the last one!)\n",
    "    for level in data[feature].unique()[0:-1]:\n",
    "        # Create new feature for this level\n",
    "        data[feature + \"_\" + level] = pd.Series(data[feature] == level, dtype=int)\n",
    "    # Drop original feature\n",
    "    data = data.drop([feature], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Minutes</th>\n",
       "      <th>Satisfaction</th>\n",
       "      <th>Churn</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Marital_Single</th>\n",
       "      <th>Marital_Married</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220</td>\n",
       "      <td>Very Low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>450</td>\n",
       "      <td>Very High</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Minutes Satisfaction  Churn  Gender_Male  Marital_Single  Marital_Married\n",
       "0      100          Low      0            1               1                0\n",
       "1      220     Very Low      0            0               0                1\n",
       "2      500         High      1            0               0                0\n",
       "3      335      Neutral      0            1               1                0\n",
       "4      450    Very High      1            1               0                1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric scaling\n",
    "We can also replace text levels with some numeric mapping we create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Satisfaction'] = data['Satisfaction'].replace(['Very Low', 'Low', 'Neutral', 'High', 'Very High'], \n",
    "                                                    [-2, -1, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Minutes</th>\n",
       "      <th>Satisfaction</th>\n",
       "      <th>Churn</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Marital_Single</th>\n",
       "      <th>Marital_Married</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>450</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Minutes  Satisfaction  Churn  Gender_Male  Marital_Single  Marital_Married\n",
       "0      100            -1      0            1               1                0\n",
       "1      220            -2      0            0               0                1\n",
       "2      500             1      1            0               0                0\n",
       "3      335             0      0            1               1                0\n",
       "4      450             2      1            1               0                1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "We are going to look at some Amazon reviews and classify them into positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The file `data/books.csv` contains 2,000 Amazon book reviews. The data set contains two features: the first column (contained in quotes) is the review text. The second column is a binary label indicating if the review is positive or negative.\n",
    "\n",
    "Let's take a quick look at the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_text,positive\r",
      "\r\n",
      "\"THis book was horrible.  If it was possible to rate it lower than one star i would have.  I am an avid reader and picked this book up after my mom had gotten it from a friend.  I read half of it, suffering from a headache the entire time, and then got to the part about the relationship the 13 year old boy had with a 33 year old man and i lit this book on fire.  One less copy in the world...don't waste your money.I wish i had the time spent reading this book back so i could use it for better purposes.  THis book wasted my life\",0\r",
      "\r\n",
      "\"I like to use the Amazon reviews when purchasing books, especially alert for dissenting perceptions about higly rated items, which usually disuades me from a selection.  So I offer this review that seriously questions the popularity of this work - I found it smug, self-serving and self-indulgent, written by a person with little or no empathy, especially for the people he castigates. For example, his portrayal of the family therapist seems implausible and reaches for effect and panders to theshrink bashers of the world. This play for effect tone throughout the book was very distasteful to me\",0\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/books.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a pandas data frame. You'll notice two new attributed in `pd.read_csv()` that we've never seen before. The first, `quotechar` is tell us what is being used to \"encapsulate\" the text fields. Since our review text is surrounding by double quotes, we let pandas know. We use a `\\` since the quote is also used to surround the quote. This backslash is known as an escape character. We also let pandas now this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/books.csv\", quotechar=\"\\\"\", escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THis book was horrible.  If it was possible to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like to use the Amazon reviews when purchasi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THis book was horrible.  If it was possible to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not sure who's writing these reviews, but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I picked up the first book in this series (The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  positive\n",
       "0  THis book was horrible.  If it was possible to...         0\n",
       "1  I like to use the Amazon reviews when purchasi...         0\n",
       "2  THis book was horrible.  If it was possible to...         0\n",
       "3  I'm not sure who's writing these reviews, but ...         0\n",
       "4  I picked up the first book in this series (The...         0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text as a set of features\n",
    "Going from text to numeric data is very easy. Let's take a look at how we can do this. We'll start by separating out our X and Y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text = data['review_text']\n",
    "Y = data['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    THis book was horrible.  If it was possible to...\n",
       "1    I like to use the Amazon reviews when purchasi...\n",
       "2    THis book was horrible.  If it was possible to...\n",
       "3    I'm not sure who's writing these reviews, but ...\n",
       "4    I picked up the first book in this series (The...\n",
       "Name: review_text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first few lines of X_text\n",
    "X_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: positive, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will turn `X_text` into just `X` -- a numeric representation that we can use in our algorithms or for queries...\n",
    "\n",
    "Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors. \n",
    "\n",
    "The result of the following is a matrix with each row a file and each column a word. The matrix is sparse because most words only appear a few times. The values are 1 if a word appears in a document and 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a vectorizer that will track text as binary features\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "binary_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = binary_vectorizer.transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 22743)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions of X:\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2000 documents (each row) and 22,743 words/tokens.\n",
    "\n",
    "Can look at some of the words by querying the binary vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hygi',\n",
       " 'hygience',\n",
       " 'hygiene',\n",
       " 'hyojin',\n",
       " 'hype',\n",
       " 'hyped',\n",
       " 'hyper',\n",
       " 'hyperbole',\n",
       " 'hyperpat',\n",
       " 'hyperpussiance',\n",
       " 'hyperspace',\n",
       " 'hypnosis',\n",
       " 'hypnotic',\n",
       " 'hypnotism',\n",
       " 'hypnotizing',\n",
       " 'hypocrisy',\n",
       " 'hypocrite',\n",
       " 'hypocritical',\n",
       " 'hypocritically',\n",
       " 'hypoglycemia']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of the 20 features (words) in column 10,000\n",
    "features = binary_vectorizer.get_feature_names()\n",
    "features[10000:10020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time to look at the binary vectoriser.\n",
    "\n",
    "Examine the structure of X. Look at some the rows and columns values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABksAAAC2CAYAAAB597puAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a210a1390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see the density of 0s and 1s in X\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.spy(X.toarray())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the sparse matrix above. Notice how some columns are quite dark (i.e. the words appear in almost every file). \n",
    "\n",
    "What are the 5 most common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 most common words are ['the', 'and', 'to', 'of', 'this']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "density_matrix = X.toarray()\n",
    "frequency = sum(density_matrix)\n",
    "#temp = copy.deepcopy(frequency)\n",
    "#temp.sort()\n",
    "temp = sorted(frequency)\n",
    "temp = temp[::-1]\n",
    "most_wordcount = temp[0:5]\n",
    "index = [np.where(frequency==a) for a in most_wordcount]\n",
    "most_words = [features[index[i][0][0]] for i in range(5)]\n",
    "print(\"the 5 most common words are \" + str(most_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes the sparse matrix X, and gets the feature list from the vectoriser, and a document index (1 - 2000) and returns a list of the words in the file that corresponds to the index (the list should be obtained from the sparse matrix / bag of words representation NOT from the original data file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13',\n",
       " '33',\n",
       " 'about',\n",
       " 'after',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'avid',\n",
       " 'back',\n",
       " 'better',\n",
       " 'book',\n",
       " 'boy',\n",
       " 'copy',\n",
       " 'could',\n",
       " 'don',\n",
       " 'entire',\n",
       " 'fire',\n",
       " 'for',\n",
       " 'friend',\n",
       " 'from',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'had',\n",
       " 'half',\n",
       " 'have',\n",
       " 'headache',\n",
       " 'horrible',\n",
       " 'if',\n",
       " 'in',\n",
       " 'it',\n",
       " 'less',\n",
       " 'life',\n",
       " 'lit',\n",
       " 'lower',\n",
       " 'man',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'my',\n",
       " 'of',\n",
       " 'old',\n",
       " 'on',\n",
       " 'one',\n",
       " 'part',\n",
       " 'picked',\n",
       " 'possible',\n",
       " 'purposes',\n",
       " 'rate',\n",
       " 'read',\n",
       " 'reader',\n",
       " 'reading',\n",
       " 'relationship',\n",
       " 'so',\n",
       " 'spent',\n",
       " 'star',\n",
       " 'suffering',\n",
       " 'than',\n",
       " 'the',\n",
       " 'then',\n",
       " 'this',\n",
       " 'time',\n",
       " 'to',\n",
       " 'up',\n",
       " 'use',\n",
       " 'was',\n",
       " 'waste',\n",
       " 'wasted',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'world',\n",
       " 'would',\n",
       " 'year',\n",
       " 'your']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete the function \n",
    "# returns vector of words / features\n",
    "def getWords(bag_of_words, file_index_row, features_list):\n",
    "    density_matrix = bag_of_words.toarray()\n",
    "    density_matrix = list(density_matrix)\n",
    "    index = []\n",
    "    for i in range(len(density_matrix[0])):\n",
    "        if density_matrix[file_index_row-1][i]==1:\n",
    "            index.append(i)\n",
    "    words = [features_list[i] for i in index]\n",
    "    return words\n",
    "getWords(X, 1, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "We have a 22743 features, let's use them in some different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of our classifier is 0.768\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "logistic_regression = LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation accuracy\n",
    "acc = model_selection.cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average accuracy rounded to three decimal points\n",
    "print (\"Mean accuracy of our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Use the above classifier to classify a new example (new review below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "the reuslt is zero which means the perdiction shows the review is negative and it seems reasonable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenggaoli/anaconda/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "new_review = \"\"\"\"\n",
    "really bad book!\n",
    "\"\"\"\n",
    "X_test = binary_vectorizer.transform([new_review]).toarray()\n",
    "clf = LogisticRegression().fit(X, Y)\n",
    "print(clf.predict(X_test))\n",
    "print(\"the reuslt is zero which means the perdiction shows the review is negative and it seems reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using full counts instead of a binary representation (i.e. each time a word appears use the raw count value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for our classifier is 0.786\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer that will track text as binary features\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "count_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = count_vectorizer.transform(X_text)\n",
    "\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation accuracy\n",
    "acc = model_selection.cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average AUC rounded to three decimal points\n",
    "print( \"Accuracy for our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try using TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for our classifier is 0.784\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer that will track text as binary features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = tfidf_vectorizer.transform(X_text)\n",
    "\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation AUCs\n",
    "acc = model_selection.cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average AUC rounded to three decimal points\n",
    "print( \"Accuracy for our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tfidf classifier to classify some online book reviews from here: https://www.amazon.com/\n",
    "\n",
    "Hint: You can copy and paste a review from the online site into a multiline string literal with 3 quotes: \n",
    "```\n",
    "\"\"\"\n",
    "copied and pasted\n",
    "multiline\n",
    "string...\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "the reuslt is zero which means the perdiction shows the review is negative and it seems reasonable\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "review1 = '''I was happy to make an additional donation in the form of a \n",
    "book that I could donate to the library, but I was sick to my stomach when I flipped \n",
    "through only a few pages of basically the same line and realized what a huge paper \n",
    "waste has been generated--and that I just contributed to--by this \n",
    "shoddy waste of time \"book.\" I love books and appreciate good humor,\n",
    "but this is neither. Please just make a direct donation to a helping \n",
    "organization and save some trees.'''\n",
    "X_test = tfidf_vectorizer.transform([review1])\n",
    "clf = LogisticRegression(solver = \"liblinear\").fit(X, Y)\n",
    "print(clf.predict(X_test))\n",
    "print(\"the reuslt is zero which means the perdiction shows the review is negative and it seems reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the implementation\n",
    "#### Features\n",
    "Tfidf is looking pretty good! How about adding n-grams? Stop words? Lowercase transforming?\n",
    "\n",
    "We saw that the most common words include \"the\" and others above - start by making these stop words.\n",
    "\n",
    "N-grams are conjunctions of words (e.g. a 2-gram adds all sequences of 2 words)\n",
    "\n",
    "\n",
    "Look at the docs: `CountVectorizer()` and `TfidfVectorizer()` can be modified to handle all of these things. Work in groups and try a few different combinations of these settings for anything you want: binary counts, numeric counts, tf-idf counts. Here is how you would use these settings:\n",
    "\n",
    "- \"`ngram_range=(1,2)`\": would include unigrams and bigrams (ie including combinations of words in sequence)\n",
    "- \"`stop_words=\"english\"`\": would use a standard set of English stop words\n",
    "- \"`lowercase=False`\": would turn off lowercase transformation (it is actually on by default)!\n",
    "\n",
    "You can use some of these like this:\n",
    "\n",
    "`tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=False)`\n",
    "\n",
    "#### Models\n",
    "Next swap out the line creating a logistic regression with one making a naive Bayes or support vector machines (SVM). SVM have been shown to be very effective in text classification. Naive Bayes has been used a lot also.\n",
    "\n",
    "For example see: http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of our classifier is 0.79\n",
      "we find that only using ngram_range get the best performance\n"
     ]
    }
   ],
   "source": [
    "# Try different features, models, or both!\n",
    "# What is the highest accuracy you can get?\n",
    "X_text = data['review_text']\n",
    "Y = data['positive']\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "X = tfidf_vectorizer.transform(X_text)\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation accuracy\n",
    "acc = model_selection.cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average accuracy rounded to three decimal points\n",
    "print (\"Mean accuracy of our classifier is \" + str(round(np.mean(acc), 3)) )\n",
    "print(\"we find that only using ngram_range get the best performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of our classifier is 0.788\n",
      "SVM do better than logistic\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "X_text = data['review_text']\n",
    "Y = data['positive']\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "X = tfidf_vectorizer.transform(X_text)\n",
    "clf = LinearSVC()\n",
    "acc = model_selection.cross_val_score(clf, X, Y, scoring=\"accuracy\", cv=5)\n",
    "print (\"Mean accuracy of our classifier is \" + str(round(np.mean(acc), 3)) )\n",
    "print(\"SVM do better than logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of our classifier is 0.813\n",
      "conmbine the features-ngram_range of vectorizer and linear SVM, the accuaracy is higher\n"
     ]
    }
   ],
   "source": [
    "X_text = data['review_text']\n",
    "Y = data['positive']\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "X = tfidf_vectorizer.transform(X_text)\n",
    "clf = LinearSVC()\n",
    "acc = model_selection.cross_val_score(clf, X, Y, scoring=\"accuracy\", cv=5)\n",
    "print (\"Mean accuracy of our classifier is \" + str(round(np.mean(acc), 3)) )\n",
    "print(\"conmbine the features-ngram_range of vectorizer and linear SVM, the accuaracy is higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of our classifier is 0.679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "X_text = data['review_text']\n",
    "Y = data['positive']\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "X = tfidf_vectorizer.transform(X_text).toarray()\n",
    "gnb = GaussianNB()\n",
    "acc = model_selection.cross_val_score(gnb, X, Y, scoring=\"accuracy\", cv=5)\n",
    "print (\"Mean accuracy of our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we can see that the performance of SVM is much better than naive bayes so we using linear SVM and only ngram_range to get the best performance.\n"
     ]
    }
   ],
   "source": [
    "print(\" we can see that the performance of SVM is much better than naive bayes so we using linear SVM and only ngram_range to get the best performance.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
